\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Agents moving in a game environment with obstacles. courtesy of supercell games.\relax }}{2}{figure.caption.5}
\contentsline {figure}{\numberline {1.2}{\ignorespaces A 6 Degrees of Freedom robotic arm moving in the 3-D surrounding space\relax }}{3}{figure.caption.6}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A RRT grown from start point [50;50]. It explores the whole state space due to the space filling property.\relax }}{9}{figure.caption.11}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Process of building the RRT and extending the current tree to $x_rand$\relax }}{10}{figure.caption.12}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A snapshot of growth of RRT starting at [50;50] and goal at [95;95]the first column is at 200 samples and second column at 2000 samples.The first row is taken when sampling randomly , second row at 10 percentage bias to goal and third row at 20 percentage bias towards goal \relax }}{12}{figure.caption.13}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Relationship between Value function , Experience and Models and DYNA framework\relax }}{24}{figure.caption.15}
\contentsline {figure}{\numberline {5.2}{\ignorespaces 15x15 grid world domain with goal and obstacles. Start state is in cell (1,4) and goal is in (11,11) cell\relax }}{31}{figure.caption.16}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Number of iterations to reach goal vs Episode for DYNA-RRTPI and RRTPI\relax }}{32}{figure.caption.17}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Value function initialized uniformly, Start point at (50,50) , goal at (95,95)\relax }}{33}{figure.caption.18}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Value function initialized using plain RRT, Start point at (50,50) , goal at (95,95)\relax }}{34}{figure.caption.19}
\contentsline {figure}{\numberline {5.6}{\ignorespaces DYNARRTPI vs RRTPI in 2-D continuous puddle world from (0,0) to (50,50) with puddle of radius 5 at (30,10) \relax }}{34}{figure.caption.20}
\contentsline {figure}{\numberline {5.7}{\ignorespaces The value function distribution for the puddle world task. The region in pink is where value function is positive, yellow represents the neutral region where as black represents the region where the value function is negative. The green line is the path learnt in RRT grown in that episode. The left graph is for 3 episodes of DYNA-RRTPI and right graph for RRTPI.\relax }}{35}{figure.caption.21}
\contentsline {figure}{\numberline {5.8}{\ignorespaces RRTPI not reaching towards the goal at very low alpha \relax }}{36}{figure.caption.22}
\contentsline {figure}{\numberline {5.9}{\ignorespaces A run of RRTPI with the modifications we proposed. \relax }}{37}{figure.caption.23}
\contentsline {figure}{\numberline {5.10}{\ignorespaces DYNA-RRTPI in successive episodes when puddle radius is increased. \relax }}{38}{figure.caption.24}
\contentsline {figure}{\numberline {5.11}{\ignorespaces from left to right, RRTPI before puddle expansion, RRTPI in the episode after puddle expansion and RRTPI after 10 episodes \relax }}{38}{figure.caption.25}
\addvspace {10\p@ }
